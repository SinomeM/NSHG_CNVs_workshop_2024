---
title: "Workshop Genome Wide CNVs in the UKB"
author: "Simone Montalbano & Andr√©s Ingason"
output:
  html_document:
    toc: true
    theme: cerulea
---

**REMEMBER!** If you run the script manually do not execute chunks marked with 
`\# NOT RUN`.


```{r setup, include = F}
# NOT RUN

knitr::opts_chunk$set(warning = FALSE, echo = TRUE, eval = TRUE,
                      include = TRUE, results = "markup", cache = T)
```

```{r remove cache, eval = F, include = F}
# NOT RUN

# not eval, run manually to remove the cached files
unlink('./02_association_analysis_cache', recursive = TRUE)
unlink('./02_association_analysis_files', recursive = TRUE)
```

```{r compile html, eval = F, include = F}
# NOT RUN

# not eval, run manually to compile into an HTML
rmarkdown::render("02_association_analysis.Rmd")
```


```{r QCtreeCNV, eval = F, include = F}
# NOT RUN

# install QCtreeCNV package from GitHub if needed
devtools::install_github("sinomem/QCtreeCNV")
```


# Very quick paranthesis, how to use CNValidatron

We will not run the program in this workshop, but in this chunk I'm
showing the code you would use to apply it to your dataset.

You will need:

1. tabix indexed intensity files (one file per sample)
2. samples files, one line per sample, links intensity file to sample_ID
3. snp list, contains name and position of the selected markers
4. cnv file, unvalidated CNV calls, from PennCNV but already processed.

In general, all required files as well as a complete PennCNV pipeline, are
described in our protocol at: 
https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621.

The protocol focuses on recurrent CNV loci, however you can just follow the
pipeline and skip all steps related to recurrent loci to have all necessary files
for CNValidatron.

```{r, eval = F}
snps <- fread('/path/to/snppos_filtered.txt')
cnvs <- fread('/path/toc/nvs_filtered.txt')
samples <- fread('/path/tol/samples_list.txt')

# folder for PNG files
png_pt <- '/path/to/folder'

# set BiocParall parallel worker limit
BiocParallel::register(BiocParallel::MulticoreParam(workers=2))
# save PNGs  
save_pngs_prediction(pred_pt, cnvs, samples, snps)

# run the prediction algoritm
preds <- make_predictions(luz::luz_load('/path/to/dropout_5_10_ukb_decode.rds'),
                          png_pt, cnvs)

# select predicted true CNVs with probability above 0.75
true_cnvs <- pred[pred %in% 2:3 & pred_prob >= 0.75, ]
```


# Data and libraries

## Libs

Load necessary libraries and CNValidatron package.

```{r libs, cache = F}
library(data.table)
library(ggplot2)

# change this to the actual path on your system
setwd('~/Documents/NSHG_CNVs_workshop_2024/02_live_session//')
```

```{r, eval = F}
# NOT RUN

# for the purpose of this analyis we pre-run all steps that require this package
# if you want to run it yourself you will need to install it.

# load the package, it will require additional installations
setwd('../../CNValidatron_fl')
devtools::load_all()
setwd('../NSHG_CNVs_workshop_2024/02_live_session')

# alternatively, just load the functions we use
# way less safe and far from optimal but should work
list.files('../CNValidatron_fl/R')
for (f in list.files('../CNValidatron_fl/R'))
  source(paste0('../CNValidatron_fl/R/', f))
```


## CNV table

Load simulated CNVs data.table.

```{r load CNVs}
cnvs <- fread('cnvs.tsv')
```


## Phenotypes

Load simulated phenotype file.

```{r load pheno}
pheno <- fread('pheno.txt')
```


# How to analyse Genome Wide CNVs

As you might have imagined from the IGV session, CNVs can be a tricky class
of variants to analyse. Their size spans from tens kilobases to few megabases
and their boundaries can vary a lot even among "similar" CNVs.

Thoughts from the homework? How would you tackle this problem? Is it even a
problem to begin with?



There are two main approaches to solve this problem. Use some kind of bins of
the genome, such as regular windows or genes, or compute CNV regions (CNVRs).
In this section we will cover both approaches.


## Binning the genome

This function can be used to count CNVs in a arbitrary set of bins across the genome.
These can be regular windows, as in this example, or any other set of segments.

```{r bin cnvs, eval = F}
# NOT RUN

source('~/Documents/CNValidatron_fl/R/binned_cnvs.R')
dt <- binned_cnvs(cnvs, format = 'count',
                  bins = CNValidatron:::binned_genome(bin_size = 250000))
saveRDS(dt, '../dev/binned_cnvs_250kbp.rds')
dt <- binned_cnvs(cnvs, format = 'count',
                  bins = CNValidatron:::binned_genome(bin_size = 100000))
saveRDS(dt, '../dev/binned_cnvs_100kbp.rds')
```

Load pre-computed object.

```{r}
bins250kb <- readRDS( '../dev/binned_cnvs_250kbp.rds')
bins100kb <- readRDS( '../dev/binned_cnvs_100kbp.rds')
```

Create BED files of the bins to check visually how they compare with CNVs, CNVRs
etc in IGV.

```{r binned BED, eval = F}
# NOT RUN

dt <- CNValidatron:::binned_genome(bin_size = 250000)[]
dt[, end := end + 2]
fwrite(dt[, .(chr, start, end, ix)], 'bed_files/bins_250.bed',
       sep ='\t', col.names = F, scipen = 1000)

dt <- CNValidatron:::binned_genome(bin_size = 100000)[]
dt[, end := end + 2]
fwrite(dt[, .(chr, start, end, ix)], 'bed_files/bins_100.bed',
       sep ='\t', col.names = F, scipen = 1000)
```


## CNVRs

A CNV region (CNVR), can be defined as a set of CNV with high internal similarity,
that is similar boundaries (start and stop) and length, in other terms, high IOU
(intersection over the union).

This function computes CNVRs using a networks based approach and the the community 
detection algorithm Leiden. It can be very memory hungry, so it been pre run.

```{r compute CNVRs, eval = F}
# NOT RUN

dt <- cnvrs_iou(cnvs, QCtreeCNV::hg19_chr_arms, min_iou = 0.50,
                max_force_merge_rounds = 5, force_merge_min_overlap = 0.75)

saveRDS(dt, '../dev/cnvrs.rds')
fwrite(dt[[1]], 'cnvs_with_cnvrs.txt', sep = '\t')
fwrite(dt[[2]], 'cnvrs.txt', sep = '\t')
```

Load pre-run objects.

```{r}
cnvs_r <- fread('cnvs_with_cnvrs.txt')
cnvrs <- fread('cnvrs.txt')
```

Create BED files for IGV.

```{r cnvrs BED, eval = F}
# NOT RUN

cnvrs[, name := paste0(CNVR, '_N', n)]

fwrite(cnvs_r[, .(chr, start, end, CNVR)],
       'bed_files/cnvs_with_cnvr.bed', sep ='\t', col.names = F)

fwrite(cnvrs[n >= 10, .(chr, start, end, name)],
       'bed_files/cnvrs_min10_freq.bed', sep ='\t', col.names = F)
```



# What about genes?

In the genome wide context it's always interesting to consider genes.
In this section we'll see how to access the ensembl database from R
and how to use genes as bins.

## Access Ensembl using biomaRt

```{r, eval = F}
# NOT RUN

library(biomaRt)

listEnsembl()

listEnsemblArchives()

searchDatasets(mart = useEnsembl(biomart = 'genes'), pattern = "hsapiens")

# create mart
ens_genes_hg19 <- useEnsembl(biomart = 'genes',
                             dataset = 'hsapiens_gene_ensembl',
                             version = 'GRCh37')
listFilters(ens_genes_hg19)[1:50, ]
listAttributes(ens_genes_hg19)[1:50, ]
as.data.table(listAttributes(ens_genes_hg19))[grep('GENCODE', description), ]
as.data.table(listAttributes(ens_genes_hg19))[grep('symbol', description), ]

# select attributes
attr <- c('ensembl_gene_id', 'hgnc_symbol', 'ensembl_transcript_id', 'chromosome_name',
          'start_position', 'end_position', 'gene_biotype', 'transcript_biotype', 'uniprotswissprot')

# query the database using attributes and filters
genes <- getBM(attributes = attr,
               filters = 'chromosome_name', values = 1:22,
               mart = ens_genes_hg19)

# save results
genes <- as.data.table(genes)
fwrite(genes, 'hg19_genes.tsv', sep = '\t')
```

Load pre-run object.

```{r}
genes <- fread('hg19_genes.tsv')
```


## Filtering genes on biotype

```{r}
dt <- unique(genes[, .(ensembl_gene_id, gene_biotype)])[, .N, by = gene_biotype]
setorder(dt, -N)
dt[]
dt <- genes[, .N, by = transcript_biotype]
setorder(dt, -N)
dt[]

genes_pc <- 
  unique(genes[gene_biotype == 'protein_coding',
                 .(ensembl_gene_id, hgnc_symbol, gene_biotype, chromosome_name,
                   start_position, end_position, gene_biotype)])

fwrite(genes_pc, 'hg19_genes_pc.tsv', sep = '\t')
```


## Use genes as bins

```{r, eval = F}
# NOT RUN

# all genes
genes <- fread('hg19_genes.tsv')

genes[, name := hgnc_symbol]
genes[name == '', name := ensembl_gene_id]
setnames(genes, c('chromosome_name', 'start_position', 'end_position'),
         c('chr', 'start', 'end'))
bin_genes <- unique(genes[, .(name, chr, start, end)])
setnames(bin_genes, 'name', 'ix')

dt <- binned_cnvs(cnvs, format = 'count', bins = bin_genes)
dt
saveRDS(dt, '../dev/binned_cnvs_all_genes.rds')

# protein coding
genes <- fread('hg19_genes_pc.tsv')

genes[, name := hgnc_symbol]
genes[name == '', name := ensembl_gene_id]
setnames(genes, c('chromosome_name', 'start_position', 'end_position'),
         c('chr', 'start', 'end'))
bin_genes <- unique(genes[, .(name, chr, start, end)])
setnames(bin_genes, 'name', 'ix')

dt <- binned_cnvs(cnvs, format = 'count', bins = bin_genes)
dt
saveRDS(dt, '../dev/binned_cnvs_pc_genes.rds')
```

Load pre-run object.

```{r}
bins_genes <- readRDS('../dev/binned_cnvs_all_genes.rds')
bins_genes <- readRDS('../dev/binned_cnvs_pc_genes.rds')
```


# The actual association analysis

Now that we have multiple ways of grouping CNVs we can try to run an association
analysis. We have simulated a phenotype with four risk loci, let's see if we can
find them all.


## Load all necessary objects

Load all pre-run objects we'll need

```{r}
setwd('~/Documents/NSHG_CNVs_workshop_2024/02_live_session//')
# read phenotype table
pheno <- fread('pheno.txt')

# grouped CNVs, regular windows, genes and CNVRs
bins250 <- readRDS("../dev/binned_cnvs_250kbp.rds")
bins100 <- readRDS("../dev/binned_cnvs_100kbp.rds")
binsgenes <- readRDS("../dev/binned_cnvs_all_genes.rds")
binsgenes_pc <- readRDS('../dev/binned_cnvs_pc_genes.rds')
cnvrs <- readRDS("../dev/cnvrs.rds")
```

## First scan

We start by running a simple Fisher's exact (FE) test
of association to identify potential risk loci across
medium sized (250kb) bins of the autosomal genome.

```{r}
# create vectors of cases and controls
cases=subset(pheno,case==1)$sample_ID
ctrls=subset(pheno,case==0)$sample_ID

# subset on minimum size of 5 carriers
preBins=subset(bins250[[2]], N>=5)
# add case status to carrier table
preCars=merge(bins250[[1]], pheno, by="sample_ID", all.x=F, all.y=F)

# define a function that returns the p-value from the FE test
fe.pval = function(bin,gt) {
  
  tmp.bin=subset(tmpCars,ix==bin & GT==gt)
  
  if (nrow(tmp.bin>0)) {
    caseCar=nrow(subset(tmp.bin, case==1))
    ctrlCar=nrow(subset(tmp.bin, case==0))
    caseNon=length(cases)-caseCar
    ctrlNon=length(ctrls)-ctrlCar
    fe.tmp=matrix(c(caseCar,caseNon,ctrlCar,ctrlNon),ncol=2)
    signif(as.numeric(fisher.test(fe.tmp)$p.value),digits=2)
  } else NA
}
 
# run function through all bins by chr (~40'')
# for (i in c(5, 8, 13, 21)) {
for (i in (1:22)) {
    print(paste("running chr",i," of 22 ..",sep=""))
    tmpBins=subset(preBins,chr==i)
    tmpCars=subset(preCars,chr==i)
    tmpBins$FEp=mapply(fe.pval, bin=tmpBins$ix, gt=tmpBins$GT)
    tmpBins=subset(tmpBins,is.na(FEp)==F)
    if (i==1) {
        tmpGWbins=tmpBins
    } else {
        tmpGWbins=rbind(tmpGWbins,tmpBins)
    }
}

# control the false discovery rate
tmpGWbins$FEpFDR=p.adjust(tmpGWbins$FEp,method="fdr")

# how many bins ?
nrow(tmpGWbins)

# was anything significant?
subset(tmpGWbins,FEpFDR<0.05)
# discovery threshold
# subset(tmpGWbins,FEpFDR<0.1)

# store results
tmpGWbins$grouping <- '250kbp_bins'
GWbins250=tmpGWbins
```


## Increase resolution

Same approach but now using smaller (100kbp) bins. This might help to detect smaller
CNVs and should give more fine-grained results.

```{r}
# subset on minimum size of 5 carriers
preBins=subset(bins100[[2]], N>=5)
# add case status to carrier table
preCars=merge(bins100[[1]], pheno, by="sample_ID", all.x=F, all.y=F)

# run function through all bins by chr (~2')
# for (i in c(5, 8, 13, 21)) {
for (i in (1:22)) {
    print(paste("running chr",i," of 22 ..",sep=""))
    tmpBins=subset(preBins,chr==i)
    tmpCars=subset(preCars,chr==i)
    tmpBins$FEp=mapply(fe.pval, bin=tmpBins$ix, gt=tmpBins$GT)
    tmpBins=subset(tmpBins,is.na(FEp)==F)
    if (i==1) {
        tmpGWbins=tmpBins
    } else {
        tmpGWbins=rbind(tmpGWbins,tmpBins)
    }
}

# control the false discovery rate
tmpGWbins$FEpFDR=p.adjust(tmpGWbins$FEp,method="fdr")

# how many bins ?
nrow(tmpGWbins)

# was anything significant ?
subset(tmpGWbins,FEpFDR<0.05)
# discovery threshold
subset(tmpGWbins,FEpFDR<0.1)

# store together with previous results
tmpGWbins$grouping <- '100kbp_bins'
GWbins100=tmpGWbins
GWbinsAll=rbind(GWbins250,GWbins100)
```

## Pause and ponder, take a look in IGV?

1. If the signal is carried by a variant smaller than the window, using smaller
   windows will improver the detection. 
2. In general, regular windows are very good for quick scan, but they lack
   specificity and granularity
3. In contrast, if the signal truly is is a large region, using small windows might
   dilute the signal
   
What can we use to improve this?





## Can CNVRs help in this situation?

```{r}
# Results up to now
subset(GWbinsAll,FEpFDR<0.05)

# subset CNVRs to those spanning the locus

# tmpBins=subset(cnvrs[[2]], chr==13 & start<47500000 & end>47250000 & n>1)
# tmpCars=subset(cnvrs[[1]], chr==13 & start<47500000 & end>47250000 & GT==1)
# tmpBins=subset(tmpBins, CNVR %in% unique(tmpCars$CNVR))
tmpBins <- cnvrs[[2]][(chr == 13 & start < 47500000 & end > 47250000) |
                         (chr == 5 & start < 61000000 & end > 60500000) |
                         (chr == 8 & start < 4250000 & end > 3750000) |
                         (chr == 21 & start < 33000000 & end > 32750000) & n > 1, ]
tmpCars <- cnvrs[[1]][CNVR %in% tmpBins$CNVR, ]


# add case status to carrier table
tmpCars=merge(tmpCars, pheno, by="sample_ID")

# estimate association across all CNVRs
resHeader=c("CNVR","ChrPos","Ncase","Nctrl","FEor","FEp")
cnvrRes=data.frame(matrix(rep(NA_real_,nrow(tmpBins)*length(resHeader)),ncol=length(resHeader)))
names(cnvrRes)=resHeader
for (i in (1:nrow(tmpBins))) {
  tmp.bin=subset(tmpCars,CNVR==tmpBins$CNVR[i])
	caseCar=nrow(subset(tmp.bin, case==1))
	ctrlCar=nrow(subset(tmp.bin, case==0))
	caseNon=length(cases)-caseCar
	ctrlNon=length(ctrls)-ctrlCar
	fe.tmp=fisher.test(matrix(c(caseCar,caseNon,ctrlCar,ctrlNon),ncol=2))
	cnvrRes$CNVR[i]=tmpBins$CNVR[i]
	cnvrRes$ChrPos[i]=paste(tmpBins$chr[i],":",tmpBins$start[i],"-",tmpBins$end[i],sep="")
	cnvrRes$Ncase[i]=caseCar
	cnvrRes$Nctrl[i]=ctrlCar
	cnvrRes$FEor[i]=round(as.numeric(fe.tmp$estimate),digits=2)
	cnvrRes$FEp[i]=signif(as.numeric(fe.tmp$p.value),digits=2)
}

# ensure that all included CNVRs have >1 carriers
cnvrRes$Ncase=as.numeric(cnvrRes$Ncase)
cnvrRes$Nctrl=as.numeric(cnvrRes$Nctrl)
cnvrRes=subset(cnvrRes,Ncase+Nctrl>1)


# .. and how do the CNVR results look
subset(cnvrRes,FEp<0.05)
```



### How do we interpret this? 

Main takeaways:  

1. one signal can be picked up by multiple bins
2. overlapping or adjacent CNVRs may contribute to one association signal
3. Other? Comments?



## What about genes?

Can we address the functional effect of CNVs more directly?
Let's try the same fine-mapping style analysis but with genes instead of CNVRs.

```{r}
# subset by-gene bins to those spanning the locus

# tmpBins=subset(binsgenes[[2]], chr==13 & start<47500000 & end>47250000 & GT==1)
# tmpCars=subset(binsgenes[[1]], chr==13 & start<47500000 & end>47250000 & GT==1)
tmpBins <- binsgenes[[2]][(chr == 13 & start < 47500000 & end > 47250000) |
                         (chr == 5 & start < 61000000 & end > 60500000) |
                         (chr == 8 & start < 4250000 & end > 3750000) |
                         (chr == 21 & start < 33000000 & end > 32750000), ]
tmpCars <- binsgenes[[1]][ix %in% tmpBins$ix, ]

# add case status to carrier table
tmpCars=merge(tmpCars, pheno, by="sample_ID")

# estimate association across all genes (del-specific)
resHeader=c("Gene","ChrPos","Ncase","Nctrl","FEor","FEp")
geneRes=data.frame(matrix(rep(NA_real_,nrow(tmpBins)*length(resHeader)),
                          ncol=length(resHeader)))
names(geneRes)=resHeader
for (i in (1:nrow(tmpBins))) {
  tmp.bin=subset(tmpCars,ix==tmpBins$ix[i])
	caseCar=nrow(subset(tmp.bin, case==1))
	ctrlCar=nrow(subset(tmp.bin, case==0))
	caseNon=length(cases)-caseCar
	ctrlNon=length(ctrls)-ctrlCar
	fe.tmp=fisher.test(matrix(c(caseCar,caseNon,ctrlCar,ctrlNon),ncol=2))
	geneRes$Gene[i]=tmpBins$ix[i]
	geneRes$ChrPos=paste(tmpBins$chr[i],":",tmpBins$start[i],"-",tmpBins$end[i],sep="")
	geneRes$Ncase[i]=caseCar
	geneRes$Nctrl[i]=ctrlCar
	geneRes$FEor[i]=round(as.numeric(fe.tmp$estimate),digits=2)
	geneRes$FEp[i]=signif(as.numeric(fe.tmp$p.value),digits=2)
}

# for recollection, how did the binned results look  
subset(GWbinsAll,FEpFDR<0.05)

# .. and the CNVR results 
subset(cnvrRes, FEp<0.05)

# .. and then what about the gene results
subset(geneRes, FEp<0.05)
```

### How to interpret the results?

- In any locus, is the gene(s) carrying the "real" risk? Is it the CNVR?
  Both, neither?
- How can we improve this analysis?



## Start over, use CNVRs directly

Let's try to use CNVRs from the very first scan to see if we detect some more signals.

Is this more likely to reflect some biologic functionality, or not ? Comments?

```{r}
# We start with CNVRs (need to accommodate for non del/dup split of CNVRs)
preBins=subset(cnvrs[[2]], n>=5)
preCars=subset(cnvrs[[1]], CNVR %in% preBins$CNVR)

# rename CNVR column to accommodate fe.pval function
names(preBins)=gsub("CNVR","ix",names(preBins))
names(preCars)=gsub("CNVR","ix",names(preCars))

# add case status  
preCars=merge(preCars, pheno, by="sample_ID", all.x=F, all.y=F)

# run function through all bins by chr (~30'')

# first deletions 
for (i in (1:22)) {
  print(paste("running chr",i," of 22 ..",sep=""))
  tmpBins=subset(preBins,chr==i)
  tmpBins$GT=1
  tmpCars=subset(preCars,chr==i & GT==1)
  tmpBins$FEp=mapply(fe.pval, bin=tmpBins$ix, gt=tmpBins$GT)
  tmpBins=subset(tmpBins,is.na(FEp)==F)
  if (i==1) {
    tmpGWbins=tmpBins
  } else {
    tmpGWbins=rbind(tmpGWbins,tmpBins)
  }
}

tmpGWbinsDel=tmpGWbins
  
# then duplications
for (i in (1:22)) {
  print(paste("running chr",i," of 22 ..",sep=""))
  tmpBins=subset(preBins,chr==i)
  tmpBins$GT=2
  tmpCars=subset(preCars,chr==i & GT==2)
  tmpBins$FEp=mapply(fe.pval, bin=tmpBins$ix, gt=tmpBins$GT)
  tmpBins=subset(tmpBins,is.na(FEp)==F)
  if (i==1) {
    tmpGWbins=tmpBins
  } else {
    tmpGWbins=rbind(tmpGWbins,tmpBins)
  }
}
tmpGWbinsDup=tmpGWbins
  
# combine del/dup results
cnvrGWbins=rbind(tmpGWbinsDel,tmpGWbinsDup)

# remove empty bins and control the false discovery rate
cnvrGWbins=subset(cnvrGWbins,is.na(FEp)==F)
cnvrGWbins$FEpFDR=p.adjust(cnvrGWbins$FEp,method="fdr")

# how many bins ?
nrow(cnvrGWbins)

# was anything significant ?
subset(cnvrGWbins,FEpFDR<0.05)

# what we got before
subset(cnvrRes, FEp<0.05)
```

No new signal, actually we get less hits than in the "fine-mapping", why is that?
Comments?


## Start over, use genes directly

We can also try using genes directly in the fist genome wide scan.

```{r}
# Now let's look at the genes
preBins=subset(binsgenes[[2]], N>=5)
preCars=subset(binsgenes[[1]], ix %in% preBins$ix)
preCars=merge(preCars,pheno,by="sample_ID", all.x=F, all.y=F)

# run function through all bins by chr (~1')
for (i in (1:22)) {
  print(paste("running chr",i," of 22 ..",sep=""))
  tmpBins=subset(preBins,chr==i)
  tmpCars=subset(preCars,chr==i)
  tmpBins$FEp=mapply(fe.pval, bin=tmpBins$ix, gt=tmpBins$GT)
  tmpBins=subset(tmpBins,is.na(FEp)==F)
  if (i==1) {
    tmpGWbins=tmpBins
  } else {
    tmpGWbins=rbind(tmpGWbins,tmpBins)
  }
}

# remove empty bins and control the false discovery rate
tmpGWbins=subset(tmpGWbins,is.na(FEp)==F)
tmpGWbins$FEpFDR=p.adjust(tmpGWbins$FEp,method="fdr")
geneGWbins=tmpGWbins

# how many bins ?
nrow(geneGWbins)

# was anything significant ?
subset(geneGWbins,FEpFDR<0.05)

# What we had before
subset(geneRes, FEp<0.05)
```

Similar results than with CNVRs, not really anything new from the fine mapping.


## Why separate deletions and duplications?

Does it make sense to analyse them together? Ideas? Comments? 

```{r}
	## finally, what if we look at del/dup combined 
	## (hypothesising they both disrupt the normal function of the overlapped gene)
	
	
    # we need to define a new function that returns the p-value from the FE test
    fe.pval.comb = function(bin) {
        tmp.bin=subset(tmpCars,ix==bin)
	    if (nrow(tmp.bin>0)) {
	      caseCar=nrow(subset(tmp.bin, case==1))
	      ctrlCar=nrow(subset(tmp.bin, case==0))
	      caseNon=length(cases)-caseCar
	      ctrlNon=length(ctrls)-ctrlCar
	      fe.tmp=matrix(c(caseCar,caseNon,ctrlCar,ctrlNon),ncol=2)
	      signif(as.numeric(fisher.test(fe.tmp)$p.value),digits=2)
	    } else {
	      NA
	    }
    }

  # We start with CNVRs (need to accommodate for non del/dup split of CNVRs)
  preBins=subset(cnvrs[[2]], n>=5)
  preCars=subset(cnvrs[[1]], CNVR %in% preBins$CNVR)

  # rename CNVR column to accommodate fe.pval function
  names(preBins)=gsub("CNVR","ix",names(preBins))
  names(preCars)=gsub("CNVR","ix",names(preCars))
  
  # add case status  
  preCars=merge(preCars, pheno, by="sample_ID", all.x=F, all.y=F)

  # run function through all bins by chr (~30'')
  for (i in (1:22)) {
      print(paste("running chr",i," of 22 ..",sep=""))
      tmpBins=subset(preBins,chr==i)
      tmpCars=subset(preCars,chr==i)
      tmpBins$FEp=lapply(tmpBins$ix, FUN=fe.pval.comb)
      tmpBins=subset(tmpBins,is.na(FEp)==F)
      if (i==1) {
          tmpGWbins=tmpBins
      } else {
          tmpGWbins=rbind(tmpGWbins,tmpBins)
      }
  }
  cnvrGWbinsComb=tmpGWbins
  
  # remove empty bins and control the false discovery rate
  cnvrGWbinsComb=subset(cnvrGWbinsComb,is.na(FEp)==F)
  cnvrGWbinsComb$FEpFDR=p.adjust(cnvrGWbinsComb$FEp,method="fdr")

  # how many bins ?
  nrow(cnvrGWbinsComb)

  # was anything significant ?
  subset(cnvrGWbinsComb,FEpFDR<0.05)
  
	
	###### NOPE, NOTHING NEW HERE

  # Finally, the same (del/dup combined) for genes
  preBins=unique(binsgenes[[2]][,1:4])
  preCars=merge(binsgenes[[1]],pheno,by="sample_ID", all.x=F, all.y=F)

  # run function through all bins by chr (~1')
  for (i in (1:22)) {
      print(paste("running chr",i," of 22 ..",sep=""))
      tmpBins=subset(preBins,chr==i)
      tmpCars=subset(preCars,chr==i)
      tmpBins$FEp=lapply(tmpBins$ix, FUN=fe.pval.comb)
      tmpBins=subset(tmpBins,is.na(FEp)==F)
      if (i==1) {
          tmpGWbins=tmpBins
      } else {
          tmpGWbins=rbind(tmpGWbins,tmpBins)
      }
  }
  
  # remove empty bins and control the false discovery rate
  tmpGWbins=subset(tmpGWbins,is.na(FEp)==F)
  tmpGWbins$FEpFDR=p.adjust(tmpGWbins$FEp,method="fdr")
  geneGWbinsComb=tmpGWbins

  # how many bins ?
  nrow(geneGWbinsComb)

  # was anything significant ?
  subset(geneGWbinsComb,FEpFDR<0.05)

  #### NO NOTHING, MAYBE AFTER SOME ADDED SIMULATION :)
  
```


# Conclusions
